import json
import os

# Ensure the notebooks directory exists
os.makedirs('/mnt/data/notebooks', exist_ok=True)

# Notebook structure
nb = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LoRA Distillation Pipeline\n",
                "\n",
                "This notebook demonstrates the full pipeline for knowledge-distillation fine-tuning of `roberta-base` on the AG News dataset using LoRA adapters.\n",
                "\n",
                "It includes:\n",
                "- Data loading & preview\n",
                "- Teacher logit generation\n",
                "- LoRA student setup\n",
                "- Distillation training with visualization\n",
                "- Parameter count\n",
                "- Final evaluation\n",
                "- Inference on unlabelled test data\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {"execution": {"iopub.execute_input": 0}},
            "source": [
                "# 1. Imports and Configuration\n",
                "import os\n",
                "import pickle\n",
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import matplotlib.pyplot as plt\n",
                "from torch.utils.data import DataLoader\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from transformers import (\n",
                "    RobertaForSequenceClassification,\n",
                "    RobertaTokenizer,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    DataCollatorWithPadding\n",
                ")\n",
                "from datasets import load_dataset\n",
                "from peft import LoraConfig, get_peft_model\n",
                "import evaluate\n",
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Global constants\n",
                "BASE_MODEL = 'roberta-base'\n",
                "NUM_EPOCHS = 3\n",
                "BATCH_SIZE_TRAIN = 16\n",
                "BATCH_SIZE_EVAL = 64\n",
                "LEARNING_RATE = 2e-4\n",
                "TEMPERATURE = 2.0\n",
                "ALPHA = 0.7\n",
                "LORA_RANK = 8\n",
                "LORA_ALPHA = 16\n",
                "LORA_DROPOUT = 0.1\n",
                "LORA_START = 6\n",
                "LORA_END = 11\n",
                "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 2. Helper Functions\n",
                "def make_lora_target_modules(start: int, end: int):\n",
                "    base_modules = ['attention.self.query', 'attention.self.value', 'output.dense']\n",
                "    targets = []\n",
                "    for layer in range(start, end + 1):\n",
                "        for m in base_modules:\n",
                "            targets.append(f'encoder.layer.{layer}.{m}')\n",
                "    return targets\n",
                "\n",
                "def distillation_loss(student_logits, teacher_logits, labels, temperature, alpha):\n",
                "    kl = F.kl_div(\n",
                "        input=F.log_softmax(student_logits / temperature, dim=-1),\n",
                "        target=F.softmax(teacher_logits / temperature, dim=-1),\n",
                "        reduction='batchmean'\n",
                "    ) * (temperature ** 2)\n",
                "    ce = F.cross_entropy(student_logits, labels)\n",
                "    return alpha * kl + (1 - alpha) * ce\n",
                "\n",
                "def compute_metrics(pred):\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    acc = evaluate.load('accuracy')\n",
                "    return acc.compute(predictions=preds, references=pred.label_ids)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 3. Data Loading & Preview\n",
                "raw = load_dataset('ag_news', split='train')\n",
                "tokenizer = RobertaTokenizer.from_pretrained(BASE_MODEL)\n",
                "\n",
                "def tokenize_fn(batch):\n",
                "    return tokenizer(batch['text'], truncation=True, padding=False)\n",
                "\n",
                "tokenized = raw.map(tokenize_fn, batched=True)\n",
                "tokenized = tokenized.rename_column('label', 'labels')\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'text': raw['text'][:5],\n",
                "    'label': raw['label'][:5]\n",
                "})\n",
                "df.head()\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 4. Teacher Logit Generation\n",
                "from src.data_utils import CustomDataset, DataCollatorWithIdx\n",
                "\n",
                "# Prepare dataset and collator\n",
                "split = tokenized.train_test_split(test_size=640, seed=42)\n",
                "train_ds = CustomDataset(split['train'])\n",
                "collator = DataCollatorWithIdx(tokenizer, return_tensors='pt')\n",
                "\n",
                "# Load teacher model\n",
                "id2label = {i: n for i, n in enumerate(raw.features['label'].names)}\n",
                "teacher = RobertaForSequenceClassification.from_pretrained(\n",
                "    BASE_MODEL, id2label=id2label, label2id={v: k for k, v in id2label.items()}\n",
                ").to(DEVICE).eval()\n",
                "\n",
                "# Generate logits\n",
                "all_logits = []\n",
                "loader = DataLoader(train_ds, batch_size=64, collate_fn=collator)\n",
                "for batch in tqdm(loader, desc='Teacher inference'):\n",
                "    idxs = batch.pop('idx')\n",
                "    inputs = {k: v.to(DEVICE) for k, v in batch.items()}\n",
                "    with torch.no_grad():\n",
                "        logits = teacher(**inputs).logits.cpu()\n",
                "    all_logits.append((idxs, logits))\n",
                "\n",
                "# Rearrange\n",
                "num_labels = raw.features['label'].num_classes\n",
                "teacher_logits = torch.zeros(len(train_ds), num_labels)\n",
                "for idxs, logits in all_logits:\n",
                "    teacher_logits[idxs] = logits\n",
                "print('Teacher logits shape:', teacher_logits.shape)\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 5. Teacher Logit Distribution\n",
                "log_vals = teacher_logits.flatten().numpy()\n",
                "plt.figure(figsize=(6,4))\n",
                "plt.hist(log_vals, bins=50)\n",
                "plt.title('Distribution of Teacher Logits')\n",
                "plt.xlabel('Logit Value')\n",
                "plt.ylabel('Frequency')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 6. Student Model & LoRA Setup\n",
                "student = RobertaForSequenceClassification.from_pretrained(\n",
                "    BASE_MODEL, id2label=id2label, label2id={v:k for k,v in id2label.items()}\n",
                ")\n",
                "target_modules = make_lora_target_modules(LORA_START, LORA_END)\n",
                "lora_cfg = LoraConfig(r=LORA_RANK, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
                "                     target_modules=target_modules, bias='none', task_type='SEQ_CLS')\n",
                "peft_model = get_peft_model(student, lora_cfg)\n",
                "peft_model.print_trainable_parameters()\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 7. Distillation Training\n",
                "split_eval = split['test']\n",
                "eval_ds = CustomDataset(split_eval)\n",
                "loader_eval = DataLoader(eval_ds, batch_size=BATCH_SIZE_EVAL, collate_fn=collator)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir='results/model_checkpoint',\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE_TRAIN,\n",
                "    per_device_eval_batch_size=BATCH_SIZE_EVAL,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    evaluation_strategy='epoch',\n",
                "    save_strategy='epoch',\n",
                "    load_best_model_at_end=True,\n",
                "    remove_unused_columns=False,\n",
                "    report_to=[]\n",
                ")\n",
                "\n",
                "def compute_loss(model, inputs, return_outputs=False):\n",
                "    labels = inputs.pop('labels')\n",
                "    idxs = inputs.pop('idx')\n",
                "    outputs = model(**inputs)\n",
                "    student_logits = outputs.logits\n",
                "    teacher_batch = teacher_logits[idxs].to(student_logits.device)\n",
                "    loss = distillation_loss(student_logits, teacher_batch, labels.to(student_logits.device), TEMPERATURE, ALPHA)\n",
                "    return (loss, outputs) if return_outputs else loss\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=peft_model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_ds,\n",
                "    eval_dataset=eval_ds,\n",
                "    data_collator=collator,\n",
                "    compute_metrics=compute_metrics,\n",
                "    compute_loss=compute_loss,\n",
                ")\n",
                "\n",
                "train_result = trainer.train()\n",
                "metrics = trainer.evaluate()\n",
                "print('Validation Accuracy:', metrics['eval_accuracy'])\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 8. Training History Visualization\n",
                "import pandas as pd\n",
                "history = pd.DataFrame(trainer.state.log_history)\n",
                "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
                "history[['loss']].plot(ax=ax[0], title='Training Loss')\n",
                "history[['eval_loss']].plot(ax=ax[0], title='Training & Eval Loss')\n",
                "history[['eval_accuracy']].plot(ax=ax[1], title='Validation Accuracy')\n",
                "plt.tight_layout()\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 9. Parameter Count\n",
                "total_params = sum(p.numel() for p in peft_model.parameters())\n",
                "trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
                "print(f'Total parameters: {total_params/1e6:.2f}M')\n",
                "print(f'Trainable parameters: {trainable_params/1e6:.2f}M')\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 10. Final Evaluation\n",
                "final_metrics, _ = evaluator = (compute_metrics, None)  # placeholder\n",
                "# we already printed eval_accuracy above\n"
            ]
        },
        {
            "cell_type": "code",
            "metadata": {},
            "source": [
                "# 11. Inference on Unlabelled Test Set\n",
                "from scripts.inference import main as run_inference\n",
                "run_inference()\n",
                "\n",
                "# Show first few lines of submission.csv\n",
                "pd.read_csv('submission.csv').head()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.x"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}

# Write the notebook to file
notebook_path = '/mnt/data/notebooks/distillation_pipeline.ipynb'
with open(notebook_path, 'w') as f:
    json.dump(nb, f, indent=2)

notebook_path

